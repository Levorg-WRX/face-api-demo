<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <title>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¡”è¨ºæ–­AI</title>
  <!-- deferã‚’å¤–ã—ã€faceapièª­ã¿è¾¼ã¿ã‚’æ˜ç¤ºçš„ã«å…ˆã«ã™ã‚‹ -->
  <script src="https://levorg-wrx.github.io/face-api-host/face-api.min.js"></script>
  <style>
    body { font-family: sans-serif; text-align: center; margin: 20px; }
    video, canvas { position: absolute; top: 0; left: 0; }
    #wrapper { position: relative; display: inline-block; }
    #result { margin-top: 280px; font-size: 1.2em; white-space: pre-line; }
    #error { margin-top: 10px; color: red; font-size: 0.9em; }
  </style>
</head>
<body>
  <h2>ğŸ“¹ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¡”è¨ºæ–­ï¼ˆå¹´é½¢ãƒ»æ€§åˆ¥ãƒ»è¡¨æƒ…ï¼‰</h2>
  <div id="wrapper">
    <video id="video" width="320" height="240" autoplay muted></video>
    <canvas id="overlay" width="320" height="240"></canvas>
  </div>
  <div id="result">ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...</div>
  <div id="error"></div>

  <script>
    document.addEventListener("DOMContentLoaded", () => {
      const MODEL_URL = "https://levorg-wrx.github.io/face-api-host/models";
      const video = document.getElementById("video");
      const canvas = document.getElementById("overlay");
      const result = document.getElementById("result");
      const errorDiv = document.getElementById("error");

      async function loadModels() {
        try {
          result.textContent = "ğŸ“¦ ssdãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­â€¦";
          await faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL);

          result.textContent = "ğŸ“¦ å¹´é½¢ãƒ»æ€§åˆ¥ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­â€¦";
          await faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL);

          result.textContent = "ğŸ“¦ è¡¨æƒ…ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­â€¦";
          await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);

          result.textContent = "ğŸ“¸ ã‚«ãƒ¡ãƒ©èµ·å‹•ä¸­...";
        } catch (e) {
          result.textContent = "âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ";
          errorDiv.textContent = e.message || e;
          throw e;
        }
      }

      async function setupCamera() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
          video.srcObject = stream;
        } catch (e) {
          result.textContent = "âŒ ã‚«ãƒ¡ãƒ©ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“";
          errorDiv.textContent = e.message || e;
          throw e;
        }
      }

      function startDetection() {
        const displaySize = { width: video.width, height: video.height };
        faceapi.matchDimensions(canvas, displaySize);

        setInterval(async () => {
          try {
            const detection = await faceapi
              .detectSingleFace(video)
              .withAgeAndGender()
              .withFaceExpressions();

            const ctx = canvas.getContext("2d");
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            if (detection) {
              const resized = faceapi.resizeResults(detection, displaySize);
              faceapi.draw.drawDetections(canvas, resized);

              const age = Math.round(detection.age);
              const gender = detection.gender === "male" ? "ç”·æ€§" : "å¥³æ€§";
              const topExp = Object.entries(detection.expressions).sort((a, b) => b[1] - a[1])[0][0];

              result.textContent = `ğŸ‘¤ å¹´é½¢ï¼š${age}æ­³\nâš§ æ€§åˆ¥ï¼š${gender}\nğŸ˜Š è¡¨æƒ…ï¼š${topExp}`;
            } else {
              result.textContent = "ğŸ˜ é¡”ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“";
            }
          } catch (e) {
            errorDiv.textContent = "è¨ºæ–­ã‚¨ãƒ©ãƒ¼: " + (e.message || e);
          }
        }, 1500);
      }

      // å®Ÿè¡Œ
      (async () => {
        try {
          await loadModels();
          await setupCamera();

          video.addEventListener("playing", () => {
            result.textContent = "ğŸ§  è¨ºæ–­ä¸­...";
            startDetection();
          });
        } catch (e) {
          console.error("åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:", e);
        }
      })();
    });
  </script>
</body>
</html>
